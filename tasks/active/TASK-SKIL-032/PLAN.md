# PLAN.md: Skill Effectiveness Tracking Implementation

**Task:** TASK-SKIL-032 - Skill Effectiveness Tracking Not Implemented
**Status:** Planning
**Created:** 2026-02-06
**Estimated Effort:** 60 minutes
**Importance:** 80 (High)

---

## 1. First Principles Analysis

### The Core Problem
The skill system has **zero ROI calculations** across all 23 defined skills. While the infrastructure exists to track skill effectiveness, the actual data collection and calculation mechanisms are not operational. This means:

- We cannot measure which skills provide value
- We cannot optimize skill selection confidence thresholds
- We cannot determine if the skill framework is worth maintaining
- We are flying blind on skill system ROI

### First Principles Breakdown

**What is skill effectiveness?**
Skill effectiveness = (Value generated by using skill) / (Cost of using skill)

**Value components:**
1. Time saved vs baseline (without skill)
2. Quality improvement in deliverables
3. Success rate of tasks
4. Reusability of skill outputs

**Cost components:**
1. Time to invoke and execute skill
2. Time to develop/maintain skill documentation
3. Cognitive overhead of skill selection

### Why Current System Fails

The current system has:
- Schema definitions for metrics (skill-metrics.yaml)
- Baseline time estimates for each skill
- Calculation formulas for all metrics
- Data structures for task outcomes

But it lacks:
- Systematic capture of "without skill" baseline data
- Automated tracking of skill vs no-skill task pairs
- Real-time metric calculation
- Integration with task completion workflow

---

## 2. Current State Assessment

### Existing Infrastructure

| Component | File | Status |
|-----------|------|--------|
| Skill definitions | operations/skill-metrics.yaml | 23 skills defined, all null metrics |
| Selection framework | operations/skill-selection.yaml | Complete, 70% threshold |
| Usage tracking | operations/skill-usage.yaml | 1 test entry only |
| Logging script | bin/log-skill-usage.py | Functional |
| Metrics calculator | bin/calculate-skill-metrics.py | Complete |
| Validation script | bin/validate-skill-usage.py | Complete |

### Current Metrics Schema

```yaml
metrics_schema:
  components:
    - success_rate (weight: 0.35)
    - time_efficiency (weight: 0.25)
    - trigger_accuracy (weight: 0.20)
    - quality_score (weight: 0.15)
    - reuse_rate (weight: 0.05)
```

### Current Data State

- **Total task outcomes tracked:** 4 (all from early testing)
- **Skills with actual usage data:** 0
- **Effectiveness scores calculated:** 0 (all null)

### What's Missing

1. **Baseline time tracking** - No comparison data for tasks without skills
2. **Automated data capture** - Manual THOUGHTS.md parsing is error-prone
3. **A/B comparison** - No paired task data (with/without skill)
4. **Real-time calculation** - Metrics only calculated on-demand

---

## 3. Proposed Effectiveness Tracking Mechanism

### Core Concept: Paired Comparison Tracking

To measure skill effectiveness, we need to compare:
- **Control group:** Tasks completed without skills (baseline)
- **Treatment group:** Tasks completed with skills

Since we cannot run true A/B tests, we will use:
1. **Historical baseline** - Tasks completed before skill invocation
2. **Estimated baseline** - Use skill's baseline_minutes from schema
3. **Post-hoc assessment** - Executor estimates "would this have taken longer without the skill?"

### Tracking Data Points

For every task, capture:

```yaml
task_outcome:
  task_id: string
  timestamp: ISO8601
  skill_used: string | null
  task_type: enum
  duration_minutes: number
  outcome: success | failure | partial
  quality_rating: 1-5
  trigger_was_correct: boolean
  would_use_again: boolean
  estimated_baseline_minutes: number  # NEW
  time_saved_minutes: number          # NEW
  notes: string
```

### Effectiveness Calculation Formula

```
effectiveness_score = weighted_average(
  success_rate * 0.35,
  time_efficiency * 0.25,
  trigger_accuracy * 0.20,
  quality_score * 0.15,
  reuse_rate * 0.05
)

time_efficiency = (estimated_baseline - actual_duration) / estimated_baseline * 100
```

---

## 4. Implementation Steps

### Phase 1: Baseline Tracking (15 min)

**Objective:** Add baseline time estimation to task workflow

**Files to modify:**
1. `bin/bb5-task` - Add prompt for estimated baseline time
2. `.autonomous/memory/hooks/task_completion_skill_recorder.py` - Add baseline capture

**Changes:**
```python
task_outcome = {
    'estimated_baseline_minutes': args.baseline or get_env_or_default('BB5_BASELINE_MINUTES'),
    'time_saved_minutes': calculate_time_saved(baseline, duration),
}
```

### Phase 2: Automated Data Capture (20 min)

**Objective:** Ensure every task records outcome data

**Files to create:**
1. `.autonomous/memory/hooks/skill-effectiveness-tracker.py` - Unified tracking hook

**Functionality:**
- Record in skill-metrics.yaml task_outcomes
- Update skill-usage.yaml if skill was used
- Calculate and update running metrics
- Generate alert if effectiveness drops below threshold

### Phase 3: Real-Time Metrics Calculation (15 min)

**Objective:** Calculate metrics automatically after each task

**Files to modify:**
1. `bin/calculate-skill-metrics.py` - Add `--continuous` mode

**Integration:**
```bash
# After task completes:
calculate-skill-metrics.py --skill $SKILL_USED --incremental
```

### Phase 4: Reporting and Alerts (10 min)

**Objective:** Make effectiveness visible

**Files to create:**
1. `bin/skill-effectiveness-dashboard.py` - Show current effectiveness scores

**Integration:**
- Add to bb5 CLI: `bb5 skill:dashboard`
- Weekly report generation

---

## 5. Files to Modify/Create

### Files to Modify

| File | Changes |
|------|---------|
| `bin/bb5-task` | Add baseline estimation prompt |
| `.autonomous/memory/hooks/task_completion_skill_recorder.py` | Add baseline capture |
| `bin/calculate-skill-metrics.py` | Add incremental calculation mode |
| `operations/skill-metrics.yaml` | Add estimated_baseline_minutes to schema |

### Files to Create

| File | Purpose |
|------|---------|
| `.autonomous/memory/hooks/skill-effectiveness-tracker.py` | Unified tracking hook |
| `bin/skill-effectiveness-dashboard.py` | Real-time dashboard |
| `.docs/skill-effectiveness-framework.md` | Documentation |

---

## 6. Success Criteria

- [ ] Every new task captures `estimated_baseline_minutes`
- [ ] Task outcomes automatically recorded on completion
- [ ] Skill metrics recalculated after each task (incremental)
- [ ] At least 10 tasks with skill usage tracked within 1 week
- [ ] At least 5 tasks without skill (baseline) tracked within 1 week
- [ ] Dashboard shows non-null effectiveness scores for used skills
- [ ] Underperforming skills (<50 effectiveness) identified

---

## 7. Rollback Strategy

If implementation causes issues:

1. **Data corruption:** Restore skill-metrics.yaml from git
2. **Hook failures:** Disable hooks in task completion workflow
3. **Performance issues:** Switch to batch calculation (nightly) instead of real-time

---

## 8. Timeline

| Phase | Duration | Cumulative |
|-------|----------|------------|
| Phase 1: Baseline Tracking | 15 min | 15 min |
| Phase 2: Automated Capture | 20 min | 35 min |
| Phase 3: Real-Time Calculation | 15 min | 50 min |
| Phase 4: Reporting | 10 min | 60 min |
| **Testing** | **15 min** | **75 min** |
| **Total** | | **~75 min** |

---

## 9. Key Design Decisions

### Decision 1: Estimated Baseline vs Actual Comparison
**Choice:** Use executor's estimated baseline rather than requiring paired tasks
**Rationale:** True A/B testing is impossible; executor judgment is sufficient for directional accuracy

### Decision 2: Incremental vs Batch Calculation
**Choice:** Support both; default to incremental
**Rationale:** Real-time feedback is valuable, but batch is more performant for large datasets

### Decision 3: Unified Hook vs Distributed Logging
**Choice:** Create unified skill-effectiveness-tracker.py hook
**Rationale:** Single point of maintenance, consistent data capture

---

*Plan created by analyzing current skill infrastructure and identifying gaps in data collection*
