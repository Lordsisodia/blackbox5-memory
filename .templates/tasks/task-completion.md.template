# Task Completion: TASK-XXX

## Summary

**Task:** FILL_ME
**Status:** Completed
**Completed By:** FILL_ME
**Completion Date:** FILL_ME

## What Was Accomplished

FILL_ME

## Files Changed

| File | Change Type | Description |
|------|-------------|-------------|
| FILL_ME | FILL_ME | FILL_ME |

## Decisions Made

- FILL_ME

## Time Invested

**Estimated:** FILL_ME
**Actual:** FILL_ME

## Lessons Learned

FILL_ME

## Follow-up Tasks

- FILL_ME

## Skill Usage Tracking

<!-- CRITICAL: This section MUST be filled out for every task -->

### Skill Selection

**Applicable skills identified:**
- [ ] Checked operations/skill-usage.yaml
- [ ] Reviewed operations/skill-selection.yaml
- [ ] Skills considered: [list or "None applicable"]

**Skill invocation decision:**
- Skill invoked: [skill name or "None"]
- Confidence level: [percentage or "N/A"]
- Rationale: [why skill was or wasn't used]

### Skill Effectiveness (if skill was used)

**Effectiveness rating:** [1-5 scale]
- 5 = Excellent - Exceeded expectations, perfect fit
- 4 = Good - Met expectations, useful assistance
- 3 = Acceptable - Some value, but not essential
- 2 = Below Average - Limited value, could have done without
- 1 = Poor - No value or hindered progress

**Time impact:**
- Estimated without skill: [minutes]
- Actual with skill: [minutes]
- Time saved/overhead: [minutes]

**Would use again for similar task:** [Yes/No/Maybe]

**Notes on skill performance:**
[Observations about what worked well or didn't]

## Testing Section

<!--
Reference: operations/testing-guidelines.yaml
           operations/.docs/testing-guide.md
-->

### Test-Driven Development (TDD)

**Did you follow TDD for this task?**
- [ ] Yes - Wrote tests before implementation
- [ ] Partial - Some tests written during/after
- [ ] No - Tests not applicable for this task type

### Tests Written

**Unit Tests:**
- [ ] Unit tests created for new functions/classes
- [ ] Test file: [path to test file]
- [ ] Coverage: [percentage or "N/A"]

**Integration Tests:**
- [ ] Integration tests created for component interactions
- [ ] Test file: [path to test file]
- [ ] Systems tested: [list systems]

**End-to-End Tests:**
- [ ] E2E tests created for complete workflows
- [ ] Test file: [path to test file]
- [ ] Workflow tested: [description]

### Test Execution

**Before Commit:**
- [ ] All new tests pass
- [ ] Existing tests still pass
- [ ] No test failures ignored
- [ ] Tests run in: [local CI / manual / other]

**Test Commands Used:**
```bash
# Example: python3 -m unittest test_module -v
# Example: pytest tests/ -v
FILL_ME or N/A
```

### Testing Checklist

- [ ] Happy path tested
- [ ] Edge cases tested
- [ ] Error conditions tested
- [ ] Async code tested (if applicable)
- [ ] Mocked external dependencies (if applicable)

## Verification Checklist

- [ ] Tests pass
- [ ] Documentation updated
- [ ] Code reviewed
- [ ] Changes committed
- [ ] Skill usage updated (if applicable)

## Roadmap Sync

<!-- AUTOMATIC - DO NOT MODIFY MANUALLY -->
**Roadmap State Sync:**
- [ ] STATE.yaml updated automatically via task-completion.yaml workflow
- [ ] Plan status synchronized (planned â†’ completed)
- [ ] Dependent plans unblocked
- [ ] next_action updated

**If sync fails:**
1. Run manual sync: `python3 lib/roadmap_sync.py --state STATE.yaml --sync-completion -t TASK-XXX`
2. Or update STATE.yaml manually:
   - Set plan status to "completed"
   - Add completed_at timestamp
   - Update next_action to next priority

<!--
SYNC_COMMAND: python3 ${RALF_ENGINE_DIR}/lib/roadmap_sync.py --state ${RALF_PROJECT_DIR}/STATE.yaml --sync-completion -t TASK-XXX -r success
-->
